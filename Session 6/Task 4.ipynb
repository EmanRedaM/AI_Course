{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6\n",
    "\n",
    "## Task 4\n",
    "\n",
    "**What is DataOps and it's tools ..**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DataOps for Data Analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**The speed and flexibility achieved by Agile and DevOps, and the quality control attained by\n",
    "SPC, can be applied to data analytics. Leading edge proponents of this approach are calling it\n",
    "DataOps. DataOps, simply stated, is Agile development and DevOps with statistical process\n",
    "control, for data analytics. DataOps applies Agile methods, DevOps, and manufacturing\n",
    "quality principles, methodologies and tools, to the data-analytics pipeline. The result is a\n",
    "rapid-response, flexible and robust data-analytics capability, which is able to keep up with\n",
    "the creativity of internal stakeholders and users.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Key business benefits of adopting DataOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Reduce time to insight\n",
    "2. Improve analytic quality\n",
    "3. Lower the marginal cost to ask the next business question\n",
    "4. Improve analytic team morale by going beyond hope, heroism and caution\n",
    "5. Promote team efficiency through agile process, reuse and refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The Seven Steps to Implement DataOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-26T12:55:27.915626Z",
     "start_time": "2021-12-26T12:55:27.899627Z"
    },
    "hidden": true
   },
   "source": [
    "<b>STEP 1 - ADD DATA AND LOGIC TESTS</b>  \n",
    "For every step in the data-analytics pipeline, there should be at least one test. The philosophy is to start with simple tests and grow over time. Even a simple test will eventually catch\n",
    "an error before it is released out to the users. For example, just making sure that row counts\n",
    "are consistent throughout the process can be a very powerful test. One could easily make\n",
    "a mistake on a join and make a cross product which fails to execute correctly. A simple rowcount test would quickly catch that.\n",
    "<br>\n",
    "    \n",
    "<b>STEP 2 - USE A VERSION CONTROL SYSTEM</b>\n",
    "<br>\n",
    "There are many processing steps that turn raw data into useful information for stakeholders.\n",
    "To be valuable, data must progress through these steps, linked together in some way, with\n",
    "the ultimate goal of producing a data-analytics output. Data may be preprocessed, cleaned,\n",
    "checked, transformed, combined, analyzed, and reported. Conceptually, the data-analysis\n",
    "pipeline is a set of stages implemented using a variety of tools including ETL tools, data\n",
    "science tools, self-service data prep tools, reporting tools, visualization tools and more.\n",
    "The stages may be executed serially, but many stages can be parallelized. The pipeline is\n",
    "deterministic because the pipeline stages are defined by scripts, source code, algorithms,\n",
    "html, configuration files, parameter files, containers and other files. All of these items are\n",
    "essentially just code. Code controls the entire data-analytics pipeline from end to end in a\n",
    "reproducible fashion.\n",
    "<br>\n",
    "    \n",
    "<b>STEP 3 - BRANCH AND MERGE</b> \n",
    "<br>\n",
    "In a typical software project, developers are continuously updating various code source files.\n",
    "If a developer wants to work on a feature, he or she pulls a copy of all relevant code from the\n",
    "version control tool and starts to develop changes on a local copy. This local copy is called a\n",
    "branch. This approach can help data-analytics teams maintain many coding changes to the\n",
    "data-analytics pipeline in parallel. When the changes to a branch are complete and tested,\n",
    "the code from the branch is merged back into the trunk, where the code came from. \n",
    "<br>\n",
    "\n",
    "<b>STEP 4 - USE MULTIPLE ENVIRONMENTS</b>\n",
    "<br>\n",
    "Every data-analytics team member has their own development tools on their own laptop.\n",
    "Version control tools allow team members to work on their own private copy of the source\n",
    "code while still staying coordinated with the rest of the team. In data analytics, a team member can’t be productive unless they also have a copy of the data that they need. Most use\n",
    "cases can be covered in less than a Terabyte (TB). Historically, disk space has been prohibitively expensive, but today, at less than $25 per TB per month (cloud storage), costs are now\n",
    "less significant than the opportunity cost of a team member’s time. If the data set is still too\n",
    "large, then a team member can take only the subset of data that is needed. Often the team\n",
    "member only needs a representative copy of the data for testing or developing one set of\n",
    "features.\n",
    "<br>\n",
    "\n",
    "<b>STEP 5 - REUSE & CONTAINERIZE</b>\n",
    "<br>\n",
    "Another productivity boosting method for teams is the ability to reuse and containerize\n",
    "code. Each middle step in the data-analytics pipeline receives output from a prior stage and\n",
    "provides input to the next stage. It is cumbersome to work with an entire data-analytics\n",
    "pipeline as one monolith, so it is common to break it down into smaller components. It’s\n",
    "easiest for other team members to reuse smaller components if they can be segmented or\n",
    "containerized. One popular container technology is Docker.\n",
    "<br>\n",
    "\n",
    "<b>STEP 6 - PARAMETERIZE YOUR PROCESSING</b>\n",
    "<br>\n",
    "There are cases when the data-analytic pipeline needs to be flexible enough to incorporate\n",
    "different run-time conditions. Which version of the raw data should be used? Is the data\n",
    "directed to production or testing? Should records be filtered according to some criterion\n",
    "(such as private health care data)? Should a specific set of processing steps in the workflow\n",
    "be included or not? To increase development velocity, these options need to be built into the\n",
    "pipeline. A robust pipeline design will allow the engineer or analyst to invoke or specify these\n",
    "options using parameters. In software development, a parameter is some information (e.g. a\n",
    "name, a number, an option) that is passed to a program that affects the way that it executes.\n",
    "If the data-analytic pipeline is designed with the right flexibility, it will be ready to accommodate different run-time circumstances.\n",
    "<br>\n",
    "\n",
    "<b>STEP 7: WORK WITHOUT FEAR OR HEROISM</b>\n",
    "<br>\n",
    "The DataOps enterprise puts the right set of tools and processes in place to enable data and\n",
    "new analytics to be deployed with a high level of quality. When an organization implements\n",
    "DataOps, engineers, scientists and analysts can relax because quality is assured. They can\n",
    "Work Without Fear or Heroism. DataOps accomplishes this by optimizing two key workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 9 Great DataOps Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data Pipeline Tools: \n",
    "Simply put, data pipelines provide organizations access to well-structured, reliable datasets so as to extract useful analytics and insights. This helps get data from operational and application systems into data warehouses analytical systems. Some of the most popular data pipelining tools include:\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Genie</b>\n",
    "<br>\n",
    "Developed by Netflix, the DataOps tool is an open-source engine that offers distributed job orchestration services. This tool provides RESTful APIs for developers who wish to run a wide range of jobs with Big Data, such as Hive, Hadoop, Presto, and Spark. Genie also provides APIs for metadata management in distributed processing clusters.\n",
    "<br><b>Piper</b><br>\n",
    "Piper is a package of Machine Learning based DataOps tools that enable organizations to read data more smoothly and efficiently. This solution exposes data through a set of APIs which integrate easily with digital assets of the organization. Furthermore, it merges batch and real-time to offer the best of data technologies along with detailed support. With a focus on AI, Pipper allows companies to minimize turnaround time of data operations and manages a complete software development lifecycle through its prepackaged data apps.\n",
    "<br><b>Airflow</b><br>\n",
    "Apache Airflow is an open-source DataOps platform that manages complex workflows in any organization by considering data processes as DAG (Directed Acyclic Graphs). This took was first designed by Airbnb to schedule and monitor their workflows. Now organizations can utilize this open-source tool to manage their data process on macOS, Linux, and Windows.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Automated Testing Tools:\n",
    "The second category of DataOps tools covers automated testing. Simply put, automated testing tools test and compare the actual outcomes of a software technique, versus the expected outcome. These tests are applied to repetitive tasks to identify the best methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Naveego</b><br>\n",
    "Naveego is a cloud data integration platform that allows businesses to reach accurate business decisions by integrating all company data in a regular business-centric format. This tool cleans stored data and makes it analytics-ready for data scientists. With Naveego, you can conveniently monitor and validate all your company’s stored data with security. \n",
    "<br><b>FirstEigen</b><br>\n",
    "FirstEigen is a platform including Machine Learning tools that provide big data quality validation and data matching on the basis of self-learning. This platform learns about data quality behaviors and models using advanced ML techniques and then tests big data with just three clicks. With FirstEigen, organizations can ensure accuracy, completeness, and sanctity of their data as it moves via multiple IT platforms.\n",
    "<br><b>RightData</b><br>\n",
    "RightData is a self-service group of applications designed for achieving data quality assurance, integrity audit and continuous control along with automated validation. This suite is best suited for organizations seeking tools with automated testing and reconciliation capabilities. With RightData, you can achieve testing for Data Migration, Database Upgrades, DAP, BI, reports, and much more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data Science Model Deployment Tools\n",
    "Model deployment is basically a method in which you integrate the AI or ML data model into any existing production environment so as to make business decisions based on the data sets. This is usually the last step in the model lifecycle and therefore, it is very crucial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b>Badook</b><br>\n",
    "Badook is a popular tool among data scientists since it allows them to write automated tests for datasets used in training/testing data models. This tool not only allows them to validate data automatically but also reduces turnaround time for generating insights. \n",
    "<br><b>DataKitchen</b><br>\n",
    "One of the most popular DataOps tools, DataKitchen is best for automating and coordinating people, environments, and tools in data analytics of the entire organization. DataKitchen handles it all – from testing to orchestration, to development, and deployment. Using this platform, your organization can achieve virtually zero errors and deploy new features faster than your business. DataKitchen lets organizations spin up repetitive work environments in a matter of minutes so teams can experiment without breaking production cycles. The Quality pipeline of DataKitchen is based on three core sections; data, production, and value. It is essential to understand that with this tool, you can access pipeline with Python Code, transform it via SQL coding, design model in R, visualize in Workbook, and gain reports in form of Tableau.\n",
    "<br><b>Lentiq</b><br>\n",
    "This data model deployment tool works in a service environment for smaller teams. With Lentiq, you can run data science and data analysis at the scale of your choice in the clouds so your team can ingest real-time data, process it, and share useful insights. With Lentiq, your team can train, build, and share models within the environment and innovate without restrictions. It is suggested to use Jupyter Notebooks for training models on Lentiq. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
